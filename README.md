# PocketNarrator: Generating Children Stories Efficiently

**Status:** In Progress ðŸš§

PocketNarrator is a project for the "Efficient Methods in Machine Learning" course (Master Project, WS25/26) at the University of Hamburg.

## Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Directory Structure](#directory-structure)
- [Requirements](#requirements)
- [Installation](#installation)
  - [1. Clone the Repository](#1-clone-the-repository)
  - [2. Set Up Environment](#2-set-up-environment)
  - [3. Install Python Dependencies](#3-install-python-dependencies)
- [Usage](#usage)
  - [Training](#training)
  - [Generation](#generation)
- [Team](#team)

## Overview

This project is a systematic investigation into the architecture and components of small language models for efficient narrative generation. Our primary goal is to develop a deep understanding of the trade-offs between different architectural choices in terms of performance, computational efficiency and output quality, rather than simply building a model that can generate stories.

To achieve this, we will implement language models from scratch using PyTorch. The project will be centered around the [TinyStories dataset](https://huggingface.co/datasets/roneneldan/TinyStories), a clean, restricted-domain dataset ideal for training capable small models without requiring massive computational resources. One thing to note: This dataset contains a collection of short children stories which we will make use of to build a simple language model with the goal of completing sentences (next token prediction). Firstly, we aim to replicate and optimise the paper corresponding to the dataset [TinyStories: How Small Can Language Models Be and Still Speak Coherent English?](https://arxiv.org/abs/2305.07759) (Eldan, et al.) by using a Decoder-only Transformer architecture based on [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (Vaswani, et al.) and [Improving Language Understanding by Generative Pre-Training](https://www.mikecaptain.com/resources/pdf/GPT-1.pdf) (Radford, et al.). Inside this architecture we will do comparisons e.g. of Tokenizers (BPE & RoPE) or different Attention mechanisms like Scaled dot-product Attention and Linear Attention. If time permits, we will compare the Transformer architecture with Mamba, a state-space model proposed in [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752) (Gu, et al.).

## Features

- **No features yet**: the features will be added alongside the course of the project

## Directory Structure

```bash
pocket-narrator/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ configs/                             # Configuration files for experiments
â”‚   â””â”€â”€ â€¦
â”œâ”€â”€ data/                                # Raw and processed datasets
â”‚   â””â”€â”€ â€¦
â”œâ”€â”€ models/                              # Saved model checkpoints
â”‚   â””â”€â”€ â€¦
â”œâ”€â”€ notebooks/                           # Exploratory data analysis and experimentation
â”‚   â””â”€â”€ â€¦
â”œâ”€â”€ pocket_narrator/                     # Source code for the PocketNarrator package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models                           # The core model architectures
â”‚   â”‚   â”œâ”€â”€ __init__.py                  # factory methods for getting & loading a model instance
â”‚   â”‚   â”œâ”€â”€ components                   # components needed for different architectures
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py      
â”‚   â”‚   â”‚   â”œâ”€â”€ base_pos_encoding.py     # abstract base class for positional encodings
â”‚   â”‚   â”‚   â””â”€â”€ positional_encoding.py   # sinusoidal & rotary positional encoding
â”‚   â”‚   â”œâ”€â”€ transformers                 # decoder-only transformer architecture
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py      
â”‚   â”‚   â”‚   â”œâ”€â”€ attention.py             # multihead self-attention
â”‚   â”‚   â”‚   â”œâ”€â”€ base_attention.py        # abstract base class for attention mechanisms
â”‚   â”‚   â”‚   â”œâ”€â”€ model.py                 # creating transformer models
â”‚   â”‚   â”‚   â””â”€â”€ transformer_block.py     # creating transformer blocks
â”‚   â”‚   â”œâ”€â”€ base_model.py                # abstract base class for language models
â”‚   â”‚   â””â”€â”€ ngram_model.py               # n-gram model
â”‚   â”œâ”€â”€ tokenizers                       # multiple different tokenizers
â”‚   â”‚   â”œâ”€â”€ __init__.py                  # factory method for getting a tokenizer instance
â”‚   â”‚   â”œâ”€â”€ base_tokenizer.py            # abstract base class for tokenizers
â”‚   â”‚   â”œâ”€â”€ bpe_tokenizer.py             # bpe tokenizer
â”‚   â”‚   â””â”€â”€ character_tokenizer.py       # character-level tokenizer
â”‚   â”œâ”€â”€ trainers                         # multiple different trainers for the different models
â”‚   â”‚   â”œâ”€â”€ __init__.py                  # factory method for getting a trainer instance
â”‚   â”‚   â”œâ”€â”€ base_trainer.py              # abstract base class for trainers
â”‚   â”‚   â”œâ”€â”€ ngram_trainer.py             # trainer for n-gram models
â”‚   â”‚   â””â”€â”€ transformer_trainer.py       # trainer for transformer models
â”‚   â”œâ”€â”€ data_loader.py                   # Loads and preprocesses data for the model
â”‚   â”œâ”€â”€ evaluate.py                      # Functions for evaluating model performance
â”œâ”€â”€ scripts/                             # Standalone scripts for execution
â”‚   â”œâ”€â”€ train.py                         # Script to train the model
â”‚   â””â”€â”€ generate.py                      # Script to generate text with a trained model
â”œâ”€â”€ tests/                               # Unit and integration tests
â”‚   â””â”€â”€ test_*.py                        # Individual test files
â”œâ”€â”€ tokenizers/                          # saved trained tokenizers
â”‚   â””â”€â”€ â€¦
â”œâ”€â”€ wandb/                               # logging
â”‚   â””â”€â”€ â€¦
```

## Requirements

- none yet

## Installation

### 1. Clone the Repository

```bash
git clone https://github.com/bschink/pocket-narrator.git
cd pocket-narrator
```

### 2. Set Up Environment

```bash
# Create conda environment
conda create -n pocket-narrator

# Activate the conda environment
conda activate pocket-narrator
```

### 3. Install Python Dependencies

```bash
pip install -r requirements.txt
```

## Usage

### Training

```bash
python scripts/train.py
```

### Generation

```bash
# to run with the default prompt
python scripts/generate.py

# to provide your own prompt
python scripts/generate.py --prompt "A girl went to the"
```

## Team

Asiya Yumna, Kosar Hazrati & Benedikt Schink
