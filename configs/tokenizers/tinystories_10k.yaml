name: tinystories_10k
type: bpe

dataset:
  hf_dataset: roneneldan/TinyStories
  config: default
  split: train
  num_rows: 10000
  #num_rows: 50000     # when we actully train „large“ no more Data than 10k
  offset: 0

vocab_size: 10000 # as a real : gpt2-medium : 87Mio Parameter ---> 2576
block_size: 256
save_dir: ./tokenizers/tinystories_10k
lm_dataset_dir: ./lm_dataset/tinystories_10k
