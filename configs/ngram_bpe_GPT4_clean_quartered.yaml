run_name: "ngram_bpe_GPT4_clean_quarted"

data:
  path: "data/processed/TinyStories/TinyStoriesV2-GPT4-train.clean.quarted.txt"
  val_ratio: 0.2
  random_seed: 42
  batch_size: 2

tokenizer:
  type: "bpe"
  path: "tokenizers/bpe_tokenizer"
  tokenizer_config:
    vocab_size: 10000
    merges_per_round: 200
    special_tokens: ["<|endoftext|>"]

model:
  type: "ngram"
  n: 3

trainer:
  type: "ngram"

generation:
  strategy: "sample"
  no_repeat_ngram_size: 3

saving:
  model_dir: "models/n_gram"
  model_name: null
