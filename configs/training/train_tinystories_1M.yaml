# configs/training/tinystories_1M_train.yaml
model_config: configs/models/tinystories_1M.yaml
tokenizer_config: configs/tokenizers/tinystories_paper_10k.yaml

training_args:
  output_dir: ./results/tinystories_1M
  overwrite_output_dir: true

  num_train_epochs: 4
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  gradient_accumulation_steps: 1

  learning_rate: 3.0e-4
  weight_decay: 0.01
  warmup_steps: 1000
  lr_scheduler_type: cosine

  evaluation_strategy: steps
  eval_steps: 1000
  save_steps: 2000
  logging_steps: 200

  block_size: 512          # muss zu n_ctx passen
