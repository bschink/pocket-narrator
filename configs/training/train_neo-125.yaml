# train_xx.yaml  (Training-configuration for GPT-Neo-125M an TinyStories 2M)

name: train_neo_125M

model_config: configs/models/gpt_neo_125M.yaml
tokenizer_config: configs/tokenizers/tinystories_2M.yaml

training_args:
  output_dir: ./results/gpt_neo_125M_tinystories_2M
  overwrite_output_dir: true

  num_train_epochs: 4
  per_device_train_batch_size: 1      # Be careful; increase later if we like (e.g., 8).
  per_device_eval_batch_size: 1

  evaluation_strategy: steps
  eval_steps: 500
  save_steps: 1000
  logging_steps: 100

  learning_rate: 3.0e-4               # for 125M ok
  warmup_steps: 1000
  save_total_limit: 2

  fp16: false                         # GPU with enough VRAM -> true is posiible
  report_to: wandb                   
