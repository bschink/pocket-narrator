name: train_x_large
model_config: configs/models/gpt2_x_large.yaml
tokenizer_config: configs/tokenizers/tinystories_1M.yaml

training_args:
  output_dir: ./results/gpt2_x_large
  overwrite_output_dir: true
  num_train_epochs: 3
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  evaluation_strategy: steps
  eval_steps: 1000
  save_steps: 2000
  logging_steps: 100
  learning_rate: 2.0e-4 # Larger models should be a bit more conservative
  warmup_steps: 2000
  save_total_limit: 2
  fp16: false
  report_to: wandb