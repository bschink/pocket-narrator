model:
  vocab_size: 10000
  d_model: 512
  n_layers: 6
  max_seq_len: 256
  dropout: 0.1
  pad_token_id: 0
  max_new_tokens: 20

training:
  lm_dataset_dir: ./lm_dataset/tinystories_8k
  output_dir: ./results/mamba_tinystories_8k
  batch_size: 32
  eval_batch_size: 32
  num_epochs: 3
  learning_rate: 3.0e-4
  weight_decay: 0.01
  max_grad_norm: 1.0
  device: cuda
  log_interval: 100
  eval_interval: 1000
  seed: 42
