name: tinystories_2k
arch: bpe

dataset:
  hf_dataset: roneneldan/TinyStories
  config: default
  split: train
  num_rows: 2000
  offset: 0

# BPE vocab + LM dataset building
vocab_size: 2048
block_size: 256
special_tokens: ["<pad>", "<bos>", "<eos>", "<unk>"]
min_frequency: 2

# Where our tokenizer + LM dataset will be saved
save_dir: ./tokenizers/tinystories_2k
lm_dataset_dir: ./lm_dataset/tinystories_2k
#seq_len: 256