name: mamba_tinystories_2k_train

# Training
output_dir: ./results/mamba_tinystories_2k
num_train_epochs: 4
batch_size: 8  # 8 -->16-->32--->64

# Optim
learning_rate: 3.0e-4 # 1e-4, 3e-4, 5e-4, 1e-3
weight_decay: 0.01
grad_accum_steps: 1
max_grad_norm: 1.0

# DataLoader
num_workers: 0 # On macOS, set num_workers: 0 first to stop spawn worker crashes while debugging.
#at the first was 4 ---> we had problem for training

# Sample logging (in mamba_main.py uses these keys)
log_samples_every_n_epochs: 1
sample_max_new_tokens: 64
sample_temperature: 0.8
sample_top_k: 50
sample_prompts:
  - "Once upon a time"
  - "In a small village"
  - "The robot said"
